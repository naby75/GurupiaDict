use quick_xml::events::Event;
use quick_xml::Reader;
use regex::Regex;
use serde::{Deserialize, Serialize};
use std::env;
use std::fs::File;
use std::io::{BufReader, BufWriter, Write};
use std::sync::LazyLock;

// â”€â”€ ì •ê·œì‹ ìºì‹±: ë§¤ í˜¸ì¶œë§ˆë‹¤ ì¬ì»´íŒŒì¼ ë°©ì§€ (#3) â”€â”€

/// ì¸í¬ë°•ìŠ¤/í…œí”Œë¦¿ ì œê±°ìš©
static RE_INFOBOX: LazyLock<Regex> =
    LazyLock::new(|| Regex::new(r"(?s)\{\{[^}]*\}\}").unwrap());

/// File/Image ë§í¬ ì œê±°ìš©
static RE_FILE: LazyLock<Regex> =
    LazyLock::new(|| Regex::new(r"\[\[(?:File|íŒŒì¼|Image|ê·¸ë¦¼):[^\]]*\]\]").unwrap());

/// <ref>...</ref> íƒœê·¸ ì œê±°ìš©
static RE_REF: LazyLock<Regex> =
    LazyLock::new(|| Regex::new(r"<ref[^>]*>.*?</ref>").unwrap());

/// HTML ì£¼ì„ ì œê±°ìš©
static RE_COMMENT: LazyLock<Regex> =
    LazyLock::new(|| Regex::new(r"<!--.*?-->").unwrap());

/// HTML íƒœê·¸ ì œê±°ìš©
static RE_HTML: LazyLock<Regex> =
    LazyLock::new(|| Regex::new(r"<[^>]+>").unwrap());

/// ì—°ì† ê°œí–‰ ì •ë¦¬ìš©
static RE_NEWLINES: LazyLock<Regex> =
    LazyLock::new(|| Regex::new(r"\n{3,}").unwrap());

/// ì—°ì† ê³µë°± ì •ë¦¬ìš©
static RE_SPACES: LazyLock<Regex> =
    LazyLock::new(|| Regex::new(r" {2,}").unwrap());


/// ë‹¨ì¼ ìœ„í‚¤ë°±ê³¼ ë¬¸ì„œ ë…¸ë“œë¥¼ í‘œí˜„
#[derive(Debug, Serialize, Deserialize)]
struct WikiNode {
    title: String,
    content: String,
}

/// XMLì—ì„œ ì¶”ì¶œëœ ìœ„í‚¤ë°±ê³¼ í˜ì´ì§€ ë°ì´í„°
#[derive(Debug, Default)]
struct WikiPage {
    title: String,
    ns: String,
    text: String,
}

fn main() -> Result<(), Box<dyn std::error::Error>> {
    let args: Vec<String> = env::args().collect();
    
    if args.len() < 3 {
        eprintln!("Usage: {} <input.xml> <output.jsonl>", args[0]);
        eprintln!("\nExample:");
        eprintln!("  {} kowiki-latest-pages-articles.xml gurupia_nodes.jsonl", args[0]);
        std::process::exit(1);
    }

    let input_path = &args[1];
    let output_path = &args[2];

    println!("ğŸ¦€ GurupiaDict Parser v0.2.0");
    println!("ğŸ“– Reading: {}", input_path);
    println!("ğŸ“ Writing: {}", output_path);
    println!();

    parse_wikipedia_xml(input_path, output_path)?;

    println!("\nâœ… Parsing completed successfully!");
    Ok(())
}

/// ìœ„í‚¤ë°±ê³¼ XML ë¤í”„ë¥¼ íŒŒì‹±í•˜ì—¬ ë©”ì¸ ë„¤ì„ìŠ¤í˜ì´ìŠ¤(ns=0) ë¬¸ì„œë§Œ ì¶”ì¶œ
fn parse_wikipedia_xml(input_path: &str, output_path: &str) -> Result<(), Box<dyn std::error::Error>> {
    let file = File::open(input_path)?;
    let _file_size = file.metadata()?.len();
    let buf_reader = BufReader::new(file);
    
    let mut reader = Reader::from_reader(buf_reader);
    reader.config_mut().trim_text(true);

    let output_file = File::create(output_path)?;
    let mut writer = BufWriter::new(output_file);

    let mut buf = Vec::new();
    let mut current_page = WikiPage::default();
    let mut current_tag = String::new();
    let mut page_count = 0u64;
    let mut processed_count = 0u64;

    loop {
        match reader.read_event_into(&mut buf) {
            Ok(Event::Start(ref e)) => {
                current_tag = String::from_utf8_lossy(e.name().as_ref()).to_string();
            }
            Ok(Event::Text(e)) => {
                let text = e.unescape()?.to_string();
                
                match current_tag.as_str() {
                    "title" => current_page.title = text,
                    "ns" => current_page.ns = text,
                    "text" => current_page.text = text,
                    _ => {}
                }
            }
            Ok(Event::End(ref e)) => {
                let tag_name = String::from_utf8_lossy(e.name().as_ref()).to_string();
                
                if tag_name == "page" {
                    page_count += 1;
                    
                    // ë©”ì¸ ë„¤ì„ìŠ¤í˜ì´ìŠ¤(ns=0) ë¬¸ì„œë§Œ ì²˜ë¦¬
                    if current_page.ns == "0" && !current_page.title.is_empty() {
                        if let Some(node) = extract_wiki_node(&current_page) {
                            let json = serde_json::to_string(&node)?;
                            writeln!(writer, "{}", json)?;
                            processed_count += 1;
                            
                            if processed_count % 1000 == 0 {
                                print!("\rğŸ“Š Processed: {} articles (Total pages: {})", 
                                       processed_count, page_count);
                                std::io::stdout().flush()?;
                            }
                        }
                    }
                    
                    // ë‹¤ìŒ í˜ì´ì§€ë¥¼ ìœ„í•´ ì´ˆê¸°í™”
                    current_page = WikiPage::default();
                }
            }
            Ok(Event::Eof) => break,
            Err(e) => {
                eprintln!("Error at position {}: {:?}", reader.buffer_position(), e);
                break;
            }
            _ => {}
        }
        buf.clear();
    }

    writer.flush()?;
    println!("\nğŸ“ˆ Final Stats:");
    println!("   Total pages scanned: {}", page_count);
    println!("   Main namespace articles extracted: {}", processed_count);

    Ok(())
}

/// ìœ„í‚¤ í˜ì´ì§€ì—ì„œ ì²« ë¬¸ë‹¨ì„ ì¶”ì¶œí•˜ê³  ì •ì œ
fn extract_wiki_node(page: &WikiPage) -> Option<WikiNode> {
    let text = &page.text;
    
    // ë¦¬ë””ë ‰íŠ¸ í˜ì´ì§€ ê±´ë„ˆë›°ê¸° (#11: í•œêµ­ì–´ '#ë„˜ê²¨ì£¼ê¸°' í¬í•¨)
    let trimmed = text.trim();
    if trimmed.starts_with("#REDIRECT")
        || trimmed.starts_with("#redirect")
        || trimmed.starts_with("#ë„˜ê²¨ì£¼ê¸°")
    {
        return None;
    }
    
    // ë™ìŒì´ì˜ì–´ í˜ì´ì§€ ê±´ë„ˆë›°ê¸°
    if page.title.contains("(ë™ìŒì´ì˜)") || text.contains("{{ë™ìŒì´ì˜}}") {
        return None;
    }

    // ì²« ë¬¸ë‹¨ ì¶”ì¶œ (ì²« ë²ˆì§¸ ì„¹ì…˜ í—¤ë” ì´ì „)
    let first_para = extract_first_paragraph(text);
    
    if first_para.is_empty() {
        return None;
    }

    // ìœ„í‚¤ ë§ˆí¬ì—… ì •ë¦¬
    let cleaned = clean_wiki_markup(&first_para);
    
    // ì½˜í…ì¸  ê¸¸ì´ ì œí•œ: 100-1500ì ë²”ìœ„, ë¬¸ì¥ ê²½ê³„ì—ì„œ ì ˆë‹¨
    let truncated = smart_truncate(&cleaned, 500, 1500);
    
    // UTF-8 char ê¸°ë°˜ ê¸¸ì´ ê²€ì¦ (#4)
    if truncated.chars().count() < 100 {
        return None;
    }

    Some(WikiNode {
        title: page.title.clone(),
        content: truncated,
    })
}

/// ì²« ë²ˆì§¸ ì„¹ì…˜ í—¤ë”(==) ì´ì „ì˜ ë„ì…ë¶€ë¥¼ ì¶”ì¶œ
fn extract_first_paragraph(text: &str) -> String {
    // ì²« ë²ˆì§¸ ì„¹ì…˜ í—¤ë” ì´ì „ ì˜ì—­ ì¶”ì¶œ
    let parts: Vec<&str> = text.split("\n==").collect();
    let intro = parts[0];
    
    // ì¸í¬ë°•ìŠ¤ ë° í…œí”Œë¦¿ ì œê±° (ìºì‹±ëœ ì •ê·œì‹ ì‚¬ìš©)
    let without_templates = RE_INFOBOX.replace_all(intro, "");
    
    // ì˜ë¯¸ ìˆëŠ” ë¬¸ë‹¨ë§Œ ì„ ë³„ (ë¹ˆ ì¤„, í…Œì´ë¸” ë§ˆí¬ì—… ì œì™¸)
    let paragraphs: Vec<&str> = without_templates
        .split("\n\n")
        .map(|p| p.trim())
        .filter(|p| !p.is_empty() && !p.starts_with("{|") && !p.starts_with("|"))
        .collect();
    
    paragraphs.join("\n\n")
}

/// ìœ„í‚¤ ë§ˆí¬ì—… ë…¸ì´ì¦ˆ ì œê±°: File ë§í¬, ì°¸ì¡°, HTML íƒœê·¸ ë“±
fn clean_wiki_markup(text: &str) -> String {
    // ëª¨ë“  ì •ê·œì‹ì€ LazyLockìœ¼ë¡œ ìºì‹±ë¨ (#3)
    let text = RE_FILE.replace_all(text, "");
    let text = RE_REF.replace_all(&text, "");
    let text = RE_COMMENT.replace_all(&text, "");
    let text = RE_HTML.replace_all(&text, "");
    let text = RE_NEWLINES.replace_all(&text, "\n\n");
    let text = RE_SPACES.replace_all(&text, " ");
    
    text.trim().to_string()
}

/// UTF-8 ì•ˆì „ ìŠ¤ë§ˆíŠ¸ ì ˆë‹¨: min~max ë²”ìœ„ì—ì„œ ë¬¸ì¥ ê²½ê³„ë¡œ ì ˆë‹¨ (#4)
fn smart_truncate(text: &str, min_chars: usize, max_chars: usize) -> String {
    let char_count = text.chars().count();
    if char_count <= max_chars {
        return text.to_string();
    }
    
    // char ì¸ë±ìŠ¤ë¥¼ ë°”ì´íŠ¸ ì¸ë±ìŠ¤ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜
    let min_byte = text.char_indices()
        .nth(min_chars)
        .map(|(i, _)| i)
        .unwrap_or(0);
    let max_byte = text.char_indices()
        .nth(max_chars)
        .map(|(i, _)| i)
        .unwrap_or(text.len());
    
    let search_range = &text[min_byte..max_byte];
    
    // í•œêµ­ì–´/ì˜ì–´ ë¬¸ì¥ ì¢…ê²° íŒ¨í„´
    let sentence_ends = [". ", ".\n", "ë‹¤.", "ë‹¤!\n", "ë‹¤?\n", "ìš”.", "ìŒ.", "ì„."];
    
    for ending in &sentence_ends {
        if let Some(pos) = search_range.rfind(ending) {
            let cut_point = min_byte + pos + ending.len();
            return text[..cut_point].trim().to_string();
        }
    }
    
    // ë¬¸ì¥ ê²½ê³„ë¥¼ ì°¾ì§€ ëª»í•˜ë©´ max_byte ìœ„ì¹˜ì—ì„œ ì ˆë‹¨
    text[..max_byte].trim().to_string()
}
